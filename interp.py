# -*- coding: utf-8 -*-
"""AliceZhangInterp-Algoverse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xmAP09pg7eJWCRxk7DqdsR5cYug7UJ5

# Interpretability (Colab T4 GPU)

**Colab setup**
1. `Runtime` -> `Change runtime type` -> Hardware accelerator: `T4 GPU`. Runtime version can remain "latest".
2. Run the installation cell below.
3. Restart the runtime if prompted, then run the verification cell.
"""

# Installation cell
!pip install transformer_lens==2.16.1 transformers==4.57.3 torch==2.9.0+cu126 numpy==1.26.4 einops==0.8.1

# Verification cell
import torch
import transformer_lens
import transformers
import einops
import numpy as np

print("torch:", torch.__version__)
print("transformers:", transformers.__version__)
print("einops:", einops.__version__)
print("numpy:", np.__version__)

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    print("GPU:", gpu_name)
    if "T4" not in gpu_name:
        print("WARNING: Expected a T4 GPU for reproducibility.")
else:
    print("WARNING: No GPU detected. Enable a T4 GPU in Colab.")

"""## Proceed with the assignment below

Be sure to use the following settings in your implementation to ensure reproducibility:
- Use `torch.manual_seed(42)` before loading the model.
- Tokenize with `prepend_bos=False`.
- Use `hook_z` and project through `W_O` manually when iterating through the layers of the transformer.

"""

torch.manual_seed(42)
model_name = "gpt2-small"
# load model
model = transformer_lens.HookedTransformer.from_pretrained(model_name, device="cuda")
# eval mode since we aren't training
model.eval()

prompt = "When Mary and John went to the store, John gave a drink to"
# tokenize
tokens = model.to_tokens(prompt, prepend_bos=False).cuda()
# just the IDs
print("Tokens:", tokens)
print(tokens.size())
# see each as a string
token_list = [model.to_string(tok) for tok in tokens[0]]
print(token_list)
print(len(token_list))

# dict for pre-W_O z values
head_z = {}
def store_z_hook(module, hook_input, hook_output, layer_idx):
    head_z[layer_idx] = hook_output.detach().clone()
hooks = []
# basically spying on the model
for i, layer in enumerate(model.blocks):
  hook = layer.attn.hook_z.register_forward_hook(lambda m, input, output, i=i: store_z_hook(m, input, output, i))
  hooks.append(hook)

# forward pass to trigger hooks
with torch.no_grad():
  logits = model(tokens)

# key = layer idx, val = corresponding tensor of all headsâ€™ contributions
per_head_residuals = {}

for layer_idx, layer in enumerate(model.blocks):
  z = head_z[layer_idx] # raw head output before W_O
  W_O = layer.attn.W_O # get the W_O
  # manually applying W_O
  per_head_residuals[layer_idx] = torch.einsum('bshd,hdf->bshf', z, W_O)

last_token_idx = -1 # should=13, corresponding to 14th token
mary_token_id = model.to_single_token(" Mary")

DLA_per_layer = {}

for layer_idx in range(model.cfg.n_layers):
  # contrib of all heads @ last tok
  heads_at_final_token = per_head_residuals[layer_idx][0, last_token_idx]
  # dot w/ unembedding vector for " Mary"
  dla_per_head = heads_at_final_token @ model.W_U[:, mary_token_id] # dla for each head in single layer @ last token
  DLA_per_layer[layer_idx] = dla_per_head.cpu().detach().numpy()

# print(dla_per_head)
print(DLA_per_layer)

max_dla = -float('inf')
min_dla = float('inf')
best_head = None
worst_head = None
dla_sum = 0
pos_dla_count = 0
layer_9_sum = 0

for layer_idx, dla_values in DLA_per_layer.items():
  for head_idx, val in enumerate(dla_values):
    if val > max_dla:
      max_dla = val
      best_head = f"Layer {layer_idx} Head {head_idx}"
    if val < min_dla:
      min_dla = val
      worst_head = f"Layer {layer_idx} Head {head_idx}"
    if val > 0:
      dla_sum += val
      pos_dla_count += 1
    if layer_idx == 9:
      layer_9_sum += abs(val)

print("Most pos DLA:", best_head, "-> value: ", round(max_dla, 4))
print("Most neg DLA:", worst_head, "-> value: ", round(min_dla, 4))
print("DLA sum (positive):", round(dla_sum, 2))
print("DLA count (positive):", pos_dla_count)
print("|DLA| sum (layer 9):", round(layer_9_sum, 2))
mary_token_id = model.to_single_token(" Mary")
print("' Mary' Token ID:", mary_token_id)

